{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianyuehz/Thompson_Sampling_MDP/blob/main/OTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iUH7vAioN5p4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing_extensions import DefaultDict\n",
        "from numpy.core.fromnumeric import argmax\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment / Util"
      ],
      "metadata": {
        "id": "RjlHPVcq_eP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zDIfHvC1ntFa"
      },
      "outputs": [],
      "source": [
        "class MDP():\n",
        "  def __init__(self, S, A, H, epLen):\n",
        "        self.S = S\n",
        "        self.A = A\n",
        "        self.H = H\n",
        "        self.epLen = epLen\n",
        "        self.P = np.random.dirichlet(np.ones(S)*0.1,(S,A,H))\n",
        "        self.R = np.ones((S,A,H)) * 0.9\n",
        "        self.R[np.random.random_sample((S,A,H)) <= 0.9] = 0 # To keep reward sparse\n",
        "        self.curr_s = np.random.choice(S)\n",
        "        self.total_reward = 0 # Cumulative reward within one episode\n",
        "\n",
        "        # Observation statistics\n",
        "        self.emperical_rewards = np.zeros((S,A,H)) \n",
        "        self.observations = np.ones((S,A,H)) \n",
        "        self.empirical_transition = np.zeros((S,A,H,S))\n",
        "\n",
        "  def step(self,a,t): \n",
        "    s = self.curr_s\n",
        "    r = self.R[s,a,t]\n",
        "    next_s = np.random.choice(self.S, p=self.P[self.curr_s,a,t])\n",
        "    self.update_stats(s,a,t,next_s, r)\n",
        "    self.curr_s = next_s\n",
        "    self.total_reward = self.total_reward + r\n",
        "\n",
        "  def update_stats(self, s, a, t, next_s,r):\n",
        "    self.emperical_rewards[s, a, t] += r\n",
        "    self.observations[s, a, t] += 1\n",
        "    self.empirical_transition[s,a,t,next_s] += 1\n",
        "\n",
        "  def reset(self): # reset statistics at the end of each episode\n",
        "    self.curr_s = np.random.choice(self.S)\n",
        "    self.total_reward = 0\n",
        "    \n",
        "      \n",
        "  def run_fixed_policy(self, p): # No learning, this works for either random, or optimal\n",
        "    r = []\n",
        "    for i in range(1000):\n",
        "      for t in range(self.H):\n",
        "        a = p[self.curr_s,t]\n",
        "        self.step(a,t)\n",
        "      r.append(self.total_reward)\n",
        "      m.reset()\n",
        "    return r\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eEPXp5s06ooD"
      },
      "outputs": [],
      "source": [
        "# If we have the access to the true reward and transition, we should be able to do good\n",
        "def optimal_policy(m): \n",
        "  S = m.S\n",
        "  H = m.H\n",
        "  A = m.A\n",
        "  policy = np.zeros((S,H),dtype=int)\n",
        "  V = np.zeros((S,H+1))\n",
        "  V[:,H] = np.zeros(S)\n",
        "  Q = np.zeros((S,A,H))\n",
        "  for t in range(H-1,-1,-1):\n",
        "    for s in range(S):\n",
        "      Q[s,:,t] = m.R[s,:,t] + np.dot(m.P[s,:,t,:], V[:, t+1])\n",
        "      policy[s,t] = np.argmax(Q[s, :, t])\n",
        "      V[s,t] = max(Q[s,:, t])\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P4W_I-cgqkEZ"
      },
      "outputs": [],
      "source": [
        "def get_avg(r):\n",
        "  window = 1000\n",
        "  average_r = []\n",
        "  for ind in range(len(r) - window + 1):\n",
        "    average_r.append(np.mean(r[ind:ind+window]))\n",
        "  for ind in range(window - 1):\n",
        "    average_r.insert(0, np.nan)\n",
        "  return average_r"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms"
      ],
      "metadata": {
        "id": "c0hjpMV9_jft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OTS"
      ],
      "metadata": {
        "id": "P0CPk04m_tX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YHRIJJUfk5QS"
      },
      "outputs": [],
      "source": [
        "# OTS\n",
        "def OTS(m):\n",
        "    S = m.S\n",
        "    H = m.H\n",
        "    A = m.A\n",
        "    T = m.epLen\n",
        "    Q = np.zeros((S,A,H))\n",
        "    V = np.zeros((S,H+1))\n",
        "    V[:,H] = np.zeros(S)\n",
        "    delta = 1/(S * A * (H**2) * (T**2))\n",
        "    policy = np.random.choice(m.A,(m.S,m.H))\n",
        "    for t in range(H-1,-1,-1):\n",
        "      for s in range(S):\n",
        "        for a in range(A):\n",
        "\n",
        "          # Compute param mu and sigma for the posterior distribution\n",
        "          mu = m.emperical_rewards[s, a, t] / m.observations[s, a, t]\n",
        "          P = m.empirical_transition[s,a,t,:] \n",
        "          norm_P = np.linalg.norm(P)\n",
        "          if norm_P != 0:\n",
        "            P = P / np.linalg.norm(P)\n",
        "          sigma = min(H, np.sqrt((S * (H ** 3) * np.log(1/delta))/(m.observations[s,a,t])))\n",
        "\n",
        "          # Sample reward, clip to empirical mean\n",
        "          r = max(mu, np.random.normal(mu, sigma))\n",
        "\n",
        "          # Compute Q \n",
        "          Q[s,a,t] = r + np.dot(P, V[:, t+1]) \n",
        "        \n",
        "        # Update policy to the best arm in this round\n",
        "        policy[s,t] = np.argmax(Q[s, :, t])\n",
        "        V[s,t] = max(Q[s,:,t])\n",
        "    return policy\n",
        "\n",
        "def run_OTS(m):\n",
        "  r = []\n",
        "  p_rand = np.random.choice(m.A,(m.S,m.H))\n",
        "  for i in tqdm(range(m.epLen)):\n",
        "    if i< 1000:\n",
        "      # Warm start for the first 1000 episode\n",
        "      for t in range(m.H):\n",
        "        a = p_rand[m.curr_s,t]\n",
        "        m.step(a,t)\n",
        "      r.append(m.total_reward)\n",
        "      m.reset()\n",
        "    else:\n",
        "      # Update policy based on new stats\n",
        "      p = OTS(m)\n",
        "      # Sampling\n",
        "      for t in range(m.H):\n",
        "        a = p[m.curr_s,t]\n",
        "        m.step(a,t)\n",
        "      r.append(m.total_reward)\n",
        "      m.reset()\n",
        "  return r"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OTS+"
      ],
      "metadata": {
        "id": "k8qMAcaD_xPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jRrJQyJrmjuU"
      },
      "outputs": [],
      "source": [
        "# OTS\n",
        "def OTS_plus(m):\n",
        "    S = m.S\n",
        "    H = m.H\n",
        "    A = m.A\n",
        "    T = m.epLen\n",
        "    Q = np.zeros((S,A,H))\n",
        "    V = np.zeros((S,H+1))\n",
        "    V[:,H] = np.zeros(S)\n",
        "    delta = 1/(S * A * (H**2) * (T**2))\n",
        "    policy = np.random.choice(m.A,(m.S,m.H))\n",
        "    for t in range(H-1,-1,-1):\n",
        "      for s in range(S):\n",
        "        for a in range(A):\n",
        "\n",
        "          # Compute param mu and sigma for the posterior distribution\n",
        "          mu = m.emperical_rewards[s, a, t] / m.observations[s, a, t]\n",
        "          P = m.empirical_transition[s,a,t,:] \n",
        "          norm_P = np.linalg.norm(P)\n",
        "          if norm_P != 0:\n",
        "            P = P / np.linalg.norm(P)\n",
        "          sigma = min(H, np.sqrt((S * (H ** 3) * np.log(1/delta))/(m.observations[s,a,t])))\n",
        "\n",
        "          # Sample reward, clip to upper confidence bound\n",
        "          ucb = mu + 2* sigma\n",
        "          r = max(ucb, np.random.normal(mu, sigma))\n",
        "\n",
        "          # Compute Q \n",
        "          Q[s,a,t] = r + np.dot(P, V[:, t+1]) \n",
        "        \n",
        "        # Update policy to the best arm in this round\n",
        "        policy[s,t] = np.argmax(Q[s, :, t])\n",
        "        V[s,t] = max(Q[s,:,t])\n",
        "    return policy\n",
        "\n",
        "def run_OTS_plus(m):\n",
        "  r = []\n",
        "  p_rand = np.random.choice(m.A,(m.S,m.H))\n",
        "  for i in tqdm(range(m.epLen)):\n",
        "    if i< 1000:\n",
        "      # Warm start for the first 1000 episode\n",
        "      for t in range(m.H):\n",
        "        a = p_rand[m.curr_s,t]\n",
        "        m.step(a,t)\n",
        "      r.append(m.total_reward)\n",
        "      m.reset()\n",
        "    else:\n",
        "      # Update policy based on new stats\n",
        "      p = OTS(m)\n",
        "      # Sampling\n",
        "      for t in range(m.H):\n",
        "        a = p[m.curr_s,t]\n",
        "        m.step(a,t)\n",
        "      r.append(m.total_reward)\n",
        "      m.reset()\n",
        "  return r"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RLSVI TODO"
      ],
      "metadata": {
        "id": "bypYao8u_1h2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ep2k4Fxjtlw9"
      },
      "outputs": [],
      "source": [
        "def RLSVI(m):\n",
        "  return\n",
        "# NARL\n",
        "# UBEV\n",
        "# UCB-VI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UBEV TODO"
      ],
      "metadata": {
        "id": "-iiR3h9UBRjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def UBEV(m):\n",
        "  return"
      ],
      "metadata": {
        "id": "QHfS_tQQBWOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NARL TODO"
      ],
      "metadata": {
        "id": "9TZOmRHkiCIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NARL(m):\n",
        "  return"
      ],
      "metadata": {
        "id": "I6VGODaRiFcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "MrV3QJ8__5B-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zqqcvr3zreIZ"
      },
      "outputs": [],
      "source": [
        "S = 3\n",
        "A = 3\n",
        "H = 5\n",
        "epLen = 1000000\n",
        "m = MDP(S,A,H,epLen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8iVxO8GLRaM",
        "outputId": "1b6354ae-2b30-4ef6-a762-42e6ce4230f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 923558/1000000 [26:21<01:51, 684.55it/s]"
          ]
        }
      ],
      "source": [
        "# Sanity check: optimal should perform better than random\n",
        "p1 = np.random.choice(m.A,(m.S,m.H))\n",
        "p2 = optimal_policy(m)\n",
        "r1 = m.run_fixed_policy(p1)\n",
        "r2 = m.run_fixed_policy(p2)\n",
        "\n",
        "r_ots = run_OTS(m) \n",
        "r_ots_plus = run_OTS_plus(m) \n",
        "\n",
        "# TODO: save results properly to avoid re-running\n",
        "plt.plot(np.ones(epLen)*np.avg(r1),label=\"random\")\n",
        "plt.plot(np.ones(epLen)*np.avg(r2),label=\"optimal\")\n",
        "plt.plot(get_avg(r_ots),label=\"OTS\")\n",
        "plt.plot(get_avg(r_ots_plus),label=\"OTS+\")\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "bypYao8u_1h2",
        "-iiR3h9UBRjO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}